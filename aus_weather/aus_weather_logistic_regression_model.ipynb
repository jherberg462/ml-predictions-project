{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the csv\n",
    "path=\"../aus_weather/weatherAUS.csv\"\n",
    "# Reading csv ->df\n",
    "df_aus_weather=pd.read_csv(path)\n",
    "df_aus_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the columns in the dataset\n",
    "df_aus_weather.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF info\n",
    "df_aus_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop records that have any null values\n",
    "df_aus_weather.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset Index\n",
    "df_aus_weather.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF info\n",
    "df_aus_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintain a copy of the df\n",
    "df_aus_weather_cp=df_aus_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the unnecessary columns\n",
    "df_aus_weather = df_aus_weather.drop(['Date', 'Location','RISK_MM','WindDir9am', 'WindDir3pm'], axis=1)\n",
    "df_aus_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aus_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aus_weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aus_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_dummies to convert categorical data (columns - RainToday and RainTomorrow)\n",
    "bin_encoded_rtod=pd.get_dummies(df_aus_weather[\"RainToday\"],drop_first=True)\n",
    "bin_encoded_rtom=pd.get_dummies(df_aus_weather[\"RainTomorrow\"],drop_first=True)\n",
    "df_aus_weather[\"RainToday\"]=bin_encoded_rtod\n",
    "df_aus_weather[\"RainTomorrow\"]=bin_encoded_rtom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify if RainToday and RainTomorrow are encoded\n",
    "df_aus_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aus_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Pandas get_dummies to convert categorical data\n",
    "df_aus_weather=pd.get_dummies(df_aus_weather)\n",
    "df_aus_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Export Pre processed Data\n",
    "# df_aus_weather.to_csv(\"preprocessed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dependencies\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aus_weather[\"RainTomorrow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and y values\n",
    "X=df_aus_weather.drop(\"RainTomorrow\",axis=1)\n",
    "y=df_aus_weather[\"RainTomorrow\"].values.reshape(-1,1)\n",
    "print(X.shape,y.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale or Normalize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Create a StandardScalar model and fit it to the training data\n",
    "X_scaler=StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the training and testing data using the X_scaler \n",
    "#Since the features are measured from different scales, appling featruring scaling\n",
    "X_train=X_scaler.transform(X_train)\n",
    "X_test=X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier=LogisticRegression()\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit(train) the model using th scaled training data\n",
    "classifier.fit(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate the model using the test data\n",
    "print(f\"Training Data Score: {classifier.score(X_train,y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance in Logistic Regression using weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Column Names\n",
    "col_names=X.columns.tolist()\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = abs(classifier.coef_[0])\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) \n",
    "featfig = plt.figure(figsize=(5,10))\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "featax.set_yticks(pos)\n",
    "featax.set_yticklabels(np.array(X.columns)[sorted_idx], fontsize=8)\n",
    "featax.set_xlabel('Relative Feature Importance')\n",
    "plt.tight_layout()   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have trained our algorithm, it’s time to make some predictions.\n",
    "y_pred=classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation using Confusion Matrix\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of the model\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Drop the columns with low feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aus_weather_cp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aus_weather_cp=df_aus_weather_cp.drop(['Date', 'Location','RISK_MM',\n",
    "                      'WindDir9am', 'WindDir3pm',\n",
    "                      'WindGustDir', 'Rainfall','Evaporation',\n",
    "                      'Humidity9am'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aus_weather_cp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_dummies to convert categorical data (columns - RainToday and RainTomorrow)\n",
    "bin_encoded_rtod1=pd.get_dummies(df_aus_weather_cp[\"RainToday\"],drop_first=True)\n",
    "bin_encoded_rtom1=pd.get_dummies(df_aus_weather_cp[\"RainTomorrow\"],drop_first=True)\n",
    "df_aus_weather_cp[\"RainToday\"]=bin_encoded_rtod1\n",
    "df_aus_weather_cp[\"RainTomorrow\"]=bin_encoded_rtom1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Pandas get_dummies to convert categorical data\n",
    "df_aus_weather_cp=pd.get_dummies(df_aus_weather_cp)\n",
    "df_aus_weather_cp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - New(updated) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and y values\n",
    "X=df_aus_weather_cp.drop(\"RainTomorrow\",axis=1)\n",
    "y=df_aus_weather_cp[\"RainTomorrow\"].values.reshape(-1,1)\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale or Normalize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Create a StandardScalar model and fit it to the training data\n",
    "X_scaler=StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the training and testing data using the X_scaler \n",
    "#Since the features are measured from different scales, appling featruring scaling\n",
    "X_train=X_scaler.transform(X_train)\n",
    "X_test=X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier_new=LogisticRegression()\n",
    "classifier_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit(train) the model using the scaled training data\n",
    "classifier.fit(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate the model using the test data\n",
    "print(f\"Training Data Score: {classifier.score(X_train,y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have trained our algorithm, it’s time to make some predictions.\n",
    "y_pred_new=classifier.predict(X_test)\n",
    "y_pred_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation using Confusion Matrix\n",
    "from sklearn import metrics\n",
    "cnf_matrix_new = metrics.confusion_matrix(y_test, y_pred_new)\n",
    "cnf_matrix_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of the model\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV to tune the model's parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C':[0.1, 0.01, 0.001, 0.0001],\n",
    "              'penalty': [\"l1\", \"l2\"]}\n",
    "grid = GridSearchCV(classifier, param_grid,n_jobs=-1, cv=5, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the hypertuned model\n",
    "predictions = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Calculate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions,\n",
    "                            target_names=[\"blue\", \"red\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeating LR model with the third dataset- weatherAUS_feature_engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the csv\n",
    "path=\"../aus_weather/weatherAUS_feature_engineer.csv\"\n",
    "# Reading csv ->df\n",
    "df_aus_weather=pd.read_csv(path)\n",
    "df_aus_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fitted model to file\n",
    "import joblib\n",
    "filename = 'logistic.sav'\n",
    "joblib.dump(grid, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
